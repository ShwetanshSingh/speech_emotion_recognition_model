{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "57ac3af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import librosa\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import keras\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "from keras.models import Sequential\n",
    "from keras.layers import (\n",
    "    Dense,\n",
    "    Conv1D,\n",
    "    MaxPooling1D,\n",
    "    Flatten,\n",
    "    Dropout,\n",
    "    BatchNormalization,\n",
    ")\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5517149f",
   "metadata": {},
   "source": [
    "### Read dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "513d810a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = pd.read_excel(\"./datasets/train_dataset.xlsx\")\n",
    "test_dataset = pd.read_excel(\"./datasets/test_dataset.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "035acfe4",
   "metadata": {},
   "source": [
    "### Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b0f9c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def noise(data):\n",
    "    \"\"\"\n",
    "    Add random Gaussian noise to the audio signal for data augmentation.\n",
    "\n",
    "    This function adds controlled random noise to the input audio signal. The amplitude\n",
    "    of the noise is proportional to the maximum amplitude of the input signal,\n",
    "    making it adaptive to different audio volumes.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : numpy.ndarray\n",
    "        Input audio signal time series\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    numpy.ndarray\n",
    "        Audio signal with added noise, same shape as input\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    The noise generation process:\n",
    "    1. Calculate noise amplitude as 3.5% of input signal's max amplitude\n",
    "    2. Generate Gaussian noise with the same length as input\n",
    "    3. Scale noise by calculated amplitude\n",
    "    4. Add scaled noise to original signal\n",
    "\n",
    "    The noise amplitude is randomized using uniform distribution to create\n",
    "    variety in the augmented data.\n",
    "    \"\"\"\n",
    "    noise_amp = 0.035 * np.random.uniform() * np.amax(data)\n",
    "    data = data + noise_amp * np.random.normal(size=data.shape[0])\n",
    "    return data\n",
    "\n",
    "\n",
    "def stretch(data, rate=0.8):\n",
    "    \"\"\"\n",
    "    Time-stretch the audio signal without changing its pitch.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : numpy.ndarray\n",
    "        Input audio signal\n",
    "    rate : float, optional\n",
    "        Stretching rate. Values > 1 speed up the audio, values < 1 slow it down.\n",
    "        Default is 0.8 (20% slower).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    numpy.ndarray\n",
    "        Time-stretched audio signal\n",
    "    \"\"\"\n",
    "    return librosa.effects.time_stretch(data, rate=rate)\n",
    "\n",
    "\n",
    "def shift(data):\n",
    "    \"\"\"\n",
    "    Randomly shift the audio signal in time.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : numpy.ndarray\n",
    "        Input audio signal\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    numpy.ndarray\n",
    "        Time-shifted audio signal, shifted by -5000 to 5000 samples\n",
    "    \"\"\"\n",
    "    shift_range = int(np.random.uniform(low=-5, high=5) * 1000)\n",
    "    return np.roll(data, shift_range)\n",
    "\n",
    "\n",
    "def pitch(data, sampling_rate, pitch_factor=0.7):\n",
    "    \"\"\"\n",
    "    Shift the pitch of the audio signal.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : numpy.ndarray\n",
    "        Input audio signal\n",
    "    sampling_rate : int\n",
    "        Sampling rate of the input audio\n",
    "    pitch_factor : float, optional\n",
    "        Number of semitones to shift. Default is 0.7 (lower pitch)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    numpy.ndarray\n",
    "        Pitch-shifted audio signal\n",
    "    \"\"\"\n",
    "    return librosa.effects.pitch_shift(data, sr=sampling_rate, n_steps=pitch_factor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f655043c",
   "metadata": {},
   "source": [
    "We use only noise and stretch, copying the steps from kaggle notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "080fed8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_audio_features(data):\n",
    "    \"\"\"\n",
    "    Extract audio features from the input audio data for emotion recognition.\n",
    "\n",
    "    This function extracts multiple audio features that are useful for speech emotion recognition:\n",
    "    - Zero Crossing Rate (ZCR): Rate at which the signal changes from positive to negative\n",
    "    - Chroma STFT: Represents the spectral energy across the 12 pitch classes\n",
    "    - MFCC (Mel-frequency cepstral coefficients): Represents the short-term power spectrum\n",
    "    - RMS (Root Mean Square): Represents the loudness of the signal\n",
    "    - Mel Spectrogram: Represents the power spectral density on mel-scale\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : numpy.ndarray\n",
    "        Audio time series data loaded using librosa\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    numpy.ndarray\n",
    "        1D array containing concatenated features in the following order:\n",
    "        [ZCR, Chroma STFT, MFCC, RMS, Mel Spectrogram]\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    All features are averaged across time using mean to get a fixed-length\n",
    "    representation regardless of the input audio length.\n",
    "    \"\"\"\n",
    "    result = np.array([])\n",
    "    zcr = np.mean(librosa.feature.zero_crossing_rate(y=data).T, axis=0)\n",
    "    result = np.hstack((result, zcr))  # stacking horizontally\n",
    "\n",
    "    stft = np.abs(librosa.stft(data))\n",
    "    chroma_stft = np.mean(librosa.feature.chroma_stft(S=stft, sr=sample_rate).T, axis=0)\n",
    "    result = np.hstack((result, chroma_stft))\n",
    "\n",
    "    mfcc = np.mean(librosa.feature.mfcc(y=data, sr=sample_rate).T, axis=0)\n",
    "    result = np.hstack((result, mfcc))\n",
    "\n",
    "    rms = np.mean(librosa.feature.rms(y=data).T, axis=0)\n",
    "    result = np.hstack((result, rms))\n",
    "\n",
    "    mel = np.mean(librosa.feature.melspectrogram(y=data, sr=sample_rate).T, axis=0)\n",
    "    result = np.hstack((result, mel))\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def augment_and_get_features(path):\n",
    "    \"\"\"\n",
    "    Load an audio file, apply data augmentation, and extract features.\n",
    "\n",
    "    This function performs the following steps:\n",
    "    1. Loads the audio file\n",
    "    2. Extracts features from the original audio\n",
    "    3. Applies noise augmentation and extracts features\n",
    "    4. Applies time stretching followed by pitch shifting and extracts features\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    path : str\n",
    "        Path to the audio file\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    numpy.ndarray\n",
    "        2D array of shape (3, n_features) containing features from:\n",
    "        - Row 0: Original audio\n",
    "        - Row 1: Noise augmented audio\n",
    "        - Row 2: Stretch and pitch augmented audio\n",
    "    \"\"\"\n",
    "    # duration and offset are used to take care of the no audio in start and the ending of each audio files as seen above.\n",
    "    data, sample_rate = librosa.load(path)\n",
    "\n",
    "    # without augmentation\n",
    "    res1 = extract_audio_features(data)\n",
    "    result = np.array(res1)\n",
    "\n",
    "    # data with noise\n",
    "    noise_data = noise(data)\n",
    "    res2 = extract_audio_features(noise_data)\n",
    "    result = np.vstack((result, res2))  # stacking vertically\n",
    "\n",
    "    # data with stretching and pitching\n",
    "    new_data = stretch(data)\n",
    "    data_stretch_pitch = pitch(new_data, sample_rate)\n",
    "    res3 = extract_audio_features(data_stretch_pitch)\n",
    "    result = np.vstack((result, res3))  # stacking vertically\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def get_features(path):\n",
    "    \"\"\"\n",
    "    Load an audio file, apply data augmentation, and extract features.\n",
    "\n",
    "    This function performs the following steps:\n",
    "    1. Loads the audio file\n",
    "    2. Extracts features from the original audio\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    path : str\n",
    "        Path to the audio file\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    numpy.ndarray\n",
    "        2D array of shape (1, n_features) containing features from:\n",
    "        - Row 0: Original audio\n",
    "    \"\"\"\n",
    "    data, sample_rate = librosa.load(path)\n",
    "\n",
    "    result = extract_audio_features(data)\n",
    "    result = np.array(result)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce00b54a",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = train_dataset[\"Path\"]\n",
    "y_train = train_dataset[\"Emotion\"]\n",
    "x_test = test_dataset[\"Path\"]\n",
    "y_test = test_dataset[\"Emotion\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "186a482f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = [], []\n",
    "for path, emotion in zip(x_train, y_train):\n",
    "    feature = augment_and_get_features(path)\n",
    "    for ele in feature:\n",
    "        X.append(ele)\n",
    "        # appending emotion 3 times as we have made 3 augmentation techniques on each audio file.\n",
    "        Y.append(emotion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a47283b",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(X), len(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d31d234",
   "metadata": {},
   "outputs": [],
   "source": [
    "Features = pd.DataFrame(X)\n",
    "Features[\"labels\"] = Y\n",
    "Features.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ff8ce8c",
   "metadata": {},
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fccf3b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_dataset = pd.read_csv(\"./datasets/test_dataset.csv\")\n",
    "# y = test_dataset[[\"Emotion\"]]\n",
    "# x = test_dataset.drop(\"Emotion\", axis=1)\n",
    "# x.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "812322ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features_from_test(path):\n",
    "    # duration and offset are used to take care of the no audio in start and the ending of each audio files as seen above.\n",
    "    data, sample_rate = librosa.load(path, duration=2.5, offset=0.6)\n",
    "\n",
    "    # without augmentation\n",
    "    res = extract_audio_features(data)\n",
    "    result = np.array(res)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "157f34fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test, y_test = [], []\n",
    "for path, emotion in zip(x.Path, y.Emotion):\n",
    "    features = get_features_from_test(path)\n",
    "    x_test.append(features)\n",
    "    y_test.append(emotion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4a61d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7402a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = pd.read_csv(\"./datasets/train_dataset.csv\")\n",
    "\n",
    "# Separate features (X) and labels (Y)\n",
    "Y = train_dataset[[\"labels\"]].copy()\n",
    "# Y.columns = ['Emotion']  # Rename the column to match our convention\n",
    "X = train_dataset.drop(\"labels\", axis=1)\n",
    "X.shape, Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a76f5e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = pd.read_csv(\"./datasets/test_dataset.csv\")\n",
    "\n",
    "y_test = test_dataset[[\"Emotion\"]].copy()\n",
    "x_test = test_dataset.drop(\"Emotion\", axis=1)\n",
    "x_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ceb9054",
   "metadata": {},
   "source": [
    "One Hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcf28c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = OneHotEncoder()\n",
    "Y = encoder.fit_transform(np.array(Y).reshape(-1, 1)).toarray()\n",
    "# y_test = encoder.fit_transform(np.array(y_test).reshape(-1, 1)).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38ca1b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = encoder.fit_transform(np.array(y_test).reshape(-1, 1)).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dd0815f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y, encoder.categories_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "814d38b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test, encoder.categories_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "775f3baf",
   "metadata": {},
   "source": [
    "Scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3073438",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a0cf835",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = scaler.fit_transform(x_test)\n",
    "x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aefb5e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cefd96c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.expand_dims(X, axis=2)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e62ab12",
   "metadata": {},
   "source": [
    "### Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74b10760",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(\n",
    "    Conv1D(\n",
    "        256,\n",
    "        kernel_size=5,\n",
    "        strides=1,\n",
    "        padding=\"same\",\n",
    "        activation=\"relu\",\n",
    "        input_shape=(X.shape[1], 1),\n",
    "    )\n",
    ")\n",
    "model.add(MaxPooling1D(pool_size=5, strides=2, padding=\"same\"))\n",
    "\n",
    "model.add(Conv1D(256, kernel_size=5, strides=1, padding=\"same\", activation=\"relu\"))\n",
    "model.add(MaxPooling1D(pool_size=5, strides=2, padding=\"same\"))\n",
    "\n",
    "model.add(Conv1D(128, kernel_size=5, strides=1, padding=\"same\", activation=\"relu\"))\n",
    "model.add(MaxPooling1D(pool_size=5, strides=2, padding=\"same\"))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Conv1D(64, kernel_size=5, strides=1, padding=\"same\", activation=\"relu\"))\n",
    "model.add(MaxPooling1D(pool_size=5, strides=2, padding=\"same\"))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(units=32, activation=\"relu\"))\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "model.add(Dense(units=8, activation=\"softmax\"))\n",
    "model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a7eb9ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "rlrp = ReduceLROnPlateau(\n",
    "    monitor=\"loss\", factor=0.4, verbose=0, patience=2, min_lr=10e-8\n",
    ")\n",
    "history = model.fit(\n",
    "    X, Y, batch_size=64, epochs=50, validation_data=(x_test, y_test), callbacks=[rlrp]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dabc7e8f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ser-training",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
