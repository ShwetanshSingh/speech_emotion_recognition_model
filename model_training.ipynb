{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "57ac3af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import librosa\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import keras\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "from keras.models import Sequential\n",
    "from keras.layers import (\n",
    "    Dense,\n",
    "    Conv1D,\n",
    "    MaxPooling1D,\n",
    "    Flatten,\n",
    "    Dropout,\n",
    "    BatchNormalization,\n",
    ")\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5517149f",
   "metadata": {},
   "source": [
    "### Read dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "513d810a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = pd.read_excel(\"./datasets/train_dataset.xlsx\")\n",
    "test_dataset = pd.read_excel(\"./datasets/test_dataset.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "035acfe4",
   "metadata": {},
   "source": [
    "### Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8b0f9c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def noise(data):\n",
    "    \"\"\"\n",
    "    Add random Gaussian noise to the audio signal for data augmentation.\n",
    "\n",
    "    This function adds controlled random noise to the input audio signal. The amplitude\n",
    "    of the noise is proportional to the maximum amplitude of the input signal,\n",
    "    making it adaptive to different audio volumes.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : numpy.ndarray\n",
    "        Input audio signal time series\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    numpy.ndarray\n",
    "        Audio signal with added noise, same shape as input\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    The noise generation process:\n",
    "    1. Calculate noise amplitude as 3.5% of input signal's max amplitude\n",
    "    2. Generate Gaussian noise with the same length as input\n",
    "    3. Scale noise by calculated amplitude\n",
    "    4. Add scaled noise to original signal\n",
    "\n",
    "    The noise amplitude is randomized using uniform distribution to create\n",
    "    variety in the augmented data.\n",
    "    \"\"\"\n",
    "    noise_amp = 0.035 * np.random.uniform() * np.amax(data)\n",
    "    data = data + noise_amp * np.random.normal(size=data.shape[0])\n",
    "    return data\n",
    "\n",
    "\n",
    "def stretch(data, rate=0.8):\n",
    "    \"\"\"\n",
    "    Time-stretch the audio signal without changing its pitch.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : numpy.ndarray\n",
    "        Input audio signal\n",
    "    rate : float, optional\n",
    "        Stretching rate. Values > 1 speed up the audio, values < 1 slow it down.\n",
    "        Default is 0.8 (20% slower).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    numpy.ndarray\n",
    "        Time-stretched audio signal\n",
    "    \"\"\"\n",
    "    return librosa.effects.time_stretch(data, rate=rate)\n",
    "\n",
    "\n",
    "def shift(data):\n",
    "    \"\"\"\n",
    "    Randomly shift the audio signal in time.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : numpy.ndarray\n",
    "        Input audio signal\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    numpy.ndarray\n",
    "        Time-shifted audio signal, shifted by -5000 to 5000 samples\n",
    "    \"\"\"\n",
    "    shift_range = int(np.random.uniform(low=-5, high=5) * 1000)\n",
    "    return np.roll(data, shift_range)\n",
    "\n",
    "\n",
    "def pitch(data, sampling_rate, pitch_factor=0.7):\n",
    "    \"\"\"\n",
    "    Shift the pitch of the audio signal.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : numpy.ndarray\n",
    "        Input audio signal\n",
    "    sampling_rate : int\n",
    "        Sampling rate of the input audio\n",
    "    pitch_factor : float, optional\n",
    "        Number of semitones to shift. Default is 0.7 (lower pitch)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    numpy.ndarray\n",
    "        Pitch-shifted audio signal\n",
    "    \"\"\"\n",
    "    return librosa.effects.pitch_shift(data, sr=sampling_rate, n_steps=pitch_factor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f655043c",
   "metadata": {},
   "source": [
    "We use only noise and stretch, copying the steps from kaggle notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "080fed8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_audio_features(data, sample_rate):\n",
    "    \"\"\"\n",
    "    Extract audio features from the input audio data for emotion recognition.\n",
    "\n",
    "    This function extracts multiple audio features that are useful for speech emotion recognition:\n",
    "    - Zero Crossing Rate (ZCR): Rate at which the signal changes from positive to negative\n",
    "    - Chroma STFT: Represents the spectral energy across the 12 pitch classes\n",
    "    - MFCC (Mel-frequency cepstral coefficients): Represents the short-term power spectrum\n",
    "    - RMS (Root Mean Square): Represents the loudness of the signal\n",
    "    - Mel Spectrogram: Represents the power spectral density on mel-scale\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : numpy.ndarray\n",
    "        Audio time series data loaded using librosa\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    numpy.ndarray\n",
    "        1D array containing concatenated features in the following order:\n",
    "        [ZCR, Chroma STFT, MFCC, RMS, Mel Spectrogram]\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    All features are averaged across time using mean to get a fixed-length\n",
    "    representation regardless of the input audio length.\n",
    "    \"\"\"\n",
    "    result = np.array([])\n",
    "    zcr = np.mean(librosa.feature.zero_crossing_rate(y=data).T, axis=0)\n",
    "    result = np.hstack((result, zcr))  # stacking horizontally\n",
    "\n",
    "    stft = np.abs(librosa.stft(data))\n",
    "    chroma_stft = np.mean(librosa.feature.chroma_stft(S=stft, sr=sample_rate).T, axis=0)\n",
    "    result = np.hstack((result, chroma_stft))\n",
    "\n",
    "    mfcc = np.mean(librosa.feature.mfcc(y=data, sr=sample_rate).T, axis=0)\n",
    "    result = np.hstack((result, mfcc))\n",
    "\n",
    "    rms = np.mean(librosa.feature.rms(y=data).T, axis=0)\n",
    "    result = np.hstack((result, rms))\n",
    "\n",
    "    mel = np.mean(librosa.feature.melspectrogram(y=data, sr=sample_rate).T, axis=0)\n",
    "    result = np.hstack((result, mel))\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def augment_and_get_features(path):\n",
    "    \"\"\"\n",
    "    Load an audio file, apply data augmentation, and extract features.\n",
    "\n",
    "    This function performs the following steps:\n",
    "    1. Loads the audio file\n",
    "    2. Extracts features from the original audio\n",
    "    3. Applies noise augmentation and extracts features\n",
    "    4. Applies time stretching followed by pitch shifting and extracts features\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    path : str\n",
    "        Path to the audio file\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    numpy.ndarray\n",
    "        2D array of shape (3, n_features) containing features from:\n",
    "        - Row 0: Original audio\n",
    "        - Row 1: Noise augmented audio\n",
    "        - Row 2: Stretch and pitch augmented audio\n",
    "    \"\"\"\n",
    "    # duration and offset are used to take care of the no audio in start and the ending of each audio files as seen above.\n",
    "    data, sample_rate = librosa.load(path)\n",
    "\n",
    "    # without augmentation\n",
    "    res1 = extract_audio_features(data, sample_rate)\n",
    "    result = np.array(res1)\n",
    "\n",
    "    # data with noise\n",
    "    noise_data = noise(data)\n",
    "    res2 = extract_audio_features(noise_data, sample_rate)\n",
    "    result = np.vstack((result, res2))  # stacking vertically\n",
    "\n",
    "    # data with stretching and pitching\n",
    "    new_data = stretch(data)\n",
    "    data_stretch_pitch = pitch(new_data, sample_rate)\n",
    "    res3 = extract_audio_features(data_stretch_pitch, sample_rate)\n",
    "    result = np.vstack((result, res3))  # stacking vertically\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def get_features(path):\n",
    "    \"\"\"\n",
    "    Load an audio file, apply data augmentation, and extract features.\n",
    "\n",
    "    This function performs the following steps:\n",
    "    1. Loads the audio file\n",
    "    2. Extracts features from the original audio\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    path : str\n",
    "        Path to the audio file\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    numpy.ndarray\n",
    "        2D array of shape (1, n_features) containing features from:\n",
    "        - Row 0: Original audio\n",
    "    \"\"\"\n",
    "    data, sample_rate = librosa.load(path)\n",
    "\n",
    "    result = extract_audio_features(data, sample_rate)\n",
    "    result = np.array(result)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce00b54a",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = train_dataset[\"Path\"]\n",
    "y_train = train_dataset[\"Emotion\"]\n",
    "x_test = test_dataset[\"Path\"]\n",
    "y_test = test_dataset[\"Emotion\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "186a482f",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_augmented, y_train_augmented = [], []\n",
    "for path, emotion in zip(x_train, y_train):\n",
    "    feature = augment_and_get_features(path)\n",
    "    for ele in feature:\n",
    "        x_train_augmented.append(ele)\n",
    "        # appending emotion 3 times as we have made 3 augmentation techniques on each audio file.\n",
    "        y_train_augmented.append(emotion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a47283b",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(x_train_augmented), len(y_train_augmented)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b89a043",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(x_train_augmented), type(y_train_augmented)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1864e78",
   "metadata": {},
   "source": [
    "no. of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b67872",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(x_train_augmented[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d31d234",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = pd.DataFrame(x_train_augmented)\n",
    "train_features[\"Emotion\"] = y_train_augmented\n",
    "train_features.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d0dd20e",
   "metadata": {},
   "source": [
    "Since, augmenting and extracting features from audio clips is a time taking process, `train_features` data can be saved in temporary directory by uncommenting the below code, to make it easier to continue later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db454d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_augmented_path = \"./datasets/temp_data/train_augmented.xlsx\"\n",
    "# train_features.to_excel(\"./datasets/temp_data/train_augmented.xlsx\", index=False)\n",
    "# train_features = pd.read_excel(train_augmented_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ff8ce8c",
   "metadata": {},
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "157f34fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test, y_test = [], []\n",
    "for path, emotion in zip(test_dataset.Path, test_dataset.Emotion):\n",
    "    features = get_features(path)\n",
    "    x_test.append(features)\n",
    "    y_test.append(emotion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4a61d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f8b99b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_features = pd.DataFrame(x_test)\n",
    "test_features[\"Emotion\"] = y_test\n",
    "test_features.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21ea12a4",
   "metadata": {},
   "source": [
    "Uncomment below code to save `test_features` data in temporary directory to continue later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b2bf8e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_features_path = \"./datasets/temp_data/test_features.xlsx\"\n",
    "# test_features.to_excel(test_features_path, index=False)\n",
    "# test_features = pd.read_excel(test_features_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b12bbeda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((27363, 162), (27363, 1))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train = train_features[[\"Emotion\"]].copy()\n",
    "y_train.columns = [\"Emotion\"]\n",
    "x_train = train_features.drop(\"Emotion\", axis=1)\n",
    "x_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "95404ab6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3041, 162), (3041, 1))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test = test_features[[\"Emotion\"]].copy()\n",
    "y_test.columns = [\"Emotion\"]\n",
    "x_test = test_features.drop(\"Emotion\", axis=1)\n",
    "x_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ceb9054",
   "metadata": {},
   "source": [
    "One Hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bcf28c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = OneHotEncoder()\n",
    "y_train = encoder.fit_transform(np.array(y_train).reshape(-1, 1)).toarray()\n",
    "y_test = encoder.fit_transform(np.array(y_test).reshape(-1, 1)).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e2284afd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 1., 0.],\n",
       "        [0., 0., 0., ..., 0., 1., 0.],\n",
       "        [0., 0., 0., ..., 0., 1., 0.]]),\n",
       " [array(['angry', 'calm', 'disgust', 'fear', 'happy', 'neutral', 'sad',\n",
       "         'surprise'], dtype=object)])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train, encoder.categories_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "775f3baf",
   "metadata": {},
   "source": [
    "Scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f3073438",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[-0.70397485,  0.06949796,  0.21696521, ..., -0.28704386,\n",
       "         -0.27498211, -0.2201443 ],\n",
       "        [ 1.40917882,  1.32117643,  1.58257157, ..., -0.10925702,\n",
       "         -0.08990944, -0.00775504],\n",
       "        [-0.35930966,  0.35376681, -0.20417907, ..., -0.28704387,\n",
       "         -0.27498213, -0.22014433],\n",
       "        ...,\n",
       "        [ 0.48491295,  0.45261468, -0.03170085, ..., -0.28704244,\n",
       "         -0.27498062, -0.22014267],\n",
       "        [ 2.06113142,  1.20598425,  1.32075349, ..., -0.09408117,\n",
       "         -0.06059889,  0.00321045],\n",
       "        [ 0.67179553, -0.25184093, -0.11018163, ..., -0.28704261,\n",
       "         -0.27498109, -0.22014387]]),\n",
       " array([[ 0.05039082, -0.34379398, -0.29185215, ..., -0.25565859,\n",
       "         -0.25299771, -0.25053177],\n",
       "        [ 0.17937001,  0.68275129,  0.81617919, ..., -0.25565855,\n",
       "         -0.25299767, -0.25053129],\n",
       "        [-0.49825688,  1.19695172,  0.86664828, ..., -0.25565854,\n",
       "         -0.25299765, -0.2505311 ],\n",
       "        ...,\n",
       "        [-0.70697849, -0.09515453, -0.28566544, ..., -0.25565764,\n",
       "         -0.25299645, -0.25051862],\n",
       "        [-0.69425193, -0.96379526, -1.21123591, ..., -0.25565543,\n",
       "         -0.2529935 , -0.25048811],\n",
       "        [ 1.69305494,  0.72156094,  0.10648434, ...,  2.82521074,\n",
       "          2.97384782,  3.01978166]]))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "x_train = scaler.fit_transform(x_train)\n",
    "x_test = scaler.fit_transform(x_test)\n",
    "x_train, x_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce04c10b",
   "metadata": {},
   "source": [
    "Expand dimensions, because model expects a specific shape of data (3D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4cefd96c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((27363, 162, 1), (3041, 162, 1))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train = np.expand_dims(x_train, axis=2)\n",
    "x_test = np.expand_dims(x_test, axis=2)\n",
    "x_train.shape, x_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e62ab12",
   "metadata": {},
   "source": [
    "### Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "74b10760",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\ml\\SER_training\\.venv\\lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ conv1d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">162</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │           <span style=\"color: #00af00; text-decoration-color: #00af00\">1,536</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ max_pooling1d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">81</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)             │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ conv1d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">81</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">327,936</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ max_pooling1d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">41</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)             │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ conv1d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">41</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">163,968</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ max_pooling1d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">21</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)             │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">21</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)             │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ conv1d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">21</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)              │          <span style=\"color: #00af00; text-decoration-color: #00af00\">41,024</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ max_pooling1d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">11</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">704</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)                  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">22,560</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)                  │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)                   │             <span style=\"color: #00af00; text-decoration-color: #00af00\">264</span> │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ conv1d (\u001b[38;5;33mConv1D\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m162\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │           \u001b[38;5;34m1,536\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ max_pooling1d (\u001b[38;5;33mMaxPooling1D\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m81\u001b[0m, \u001b[38;5;34m256\u001b[0m)             │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ conv1d_1 (\u001b[38;5;33mConv1D\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m81\u001b[0m, \u001b[38;5;34m256\u001b[0m)             │         \u001b[38;5;34m327,936\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ max_pooling1d_1 (\u001b[38;5;33mMaxPooling1D\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m41\u001b[0m, \u001b[38;5;34m256\u001b[0m)             │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ conv1d_2 (\u001b[38;5;33mConv1D\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m41\u001b[0m, \u001b[38;5;34m128\u001b[0m)             │         \u001b[38;5;34m163,968\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ max_pooling1d_2 (\u001b[38;5;33mMaxPooling1D\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m21\u001b[0m, \u001b[38;5;34m128\u001b[0m)             │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m21\u001b[0m, \u001b[38;5;34m128\u001b[0m)             │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ conv1d_3 (\u001b[38;5;33mConv1D\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m21\u001b[0m, \u001b[38;5;34m64\u001b[0m)              │          \u001b[38;5;34m41,024\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ max_pooling1d_3 (\u001b[38;5;33mMaxPooling1D\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m11\u001b[0m, \u001b[38;5;34m64\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m704\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)                  │          \u001b[38;5;34m22,560\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)                  │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m)                   │             \u001b[38;5;34m264\u001b[0m │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">557,288</span> (2.13 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m557,288\u001b[0m (2.13 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">557,288</span> (2.13 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m557,288\u001b[0m (2.13 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(\n",
    "    Conv1D(\n",
    "        256,\n",
    "        kernel_size=5,\n",
    "        strides=1,\n",
    "        padding=\"same\",\n",
    "        activation=\"relu\",\n",
    "        input_shape=(x_train.shape[1], 1),\n",
    "    )\n",
    ")\n",
    "model.add(MaxPooling1D(pool_size=5, strides=2, padding=\"same\"))\n",
    "\n",
    "model.add(Conv1D(256, kernel_size=5, strides=1, padding=\"same\", activation=\"relu\"))\n",
    "model.add(MaxPooling1D(pool_size=5, strides=2, padding=\"same\"))\n",
    "\n",
    "model.add(Conv1D(128, kernel_size=5, strides=1, padding=\"same\", activation=\"relu\"))\n",
    "model.add(MaxPooling1D(pool_size=5, strides=2, padding=\"same\"))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Conv1D(64, kernel_size=5, strides=1, padding=\"same\", activation=\"relu\"))\n",
    "model.add(MaxPooling1D(pool_size=5, strides=2, padding=\"same\"))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(units=32, activation=\"relu\"))\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "model.add(Dense(units=8, activation=\"softmax\"))\n",
    "model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2a7eb9ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "\u001b[1m428/428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 41ms/step - accuracy: 0.2529 - loss: 1.8263 - val_accuracy: 0.3992 - val_loss: 1.4964 - learning_rate: 0.0010\n",
      "Epoch 2/25\n",
      "\u001b[1m428/428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 40ms/step - accuracy: 0.3869 - loss: 1.4993 - val_accuracy: 0.4331 - val_loss: 1.3815 - learning_rate: 0.0010\n",
      "Epoch 3/25\n",
      "\u001b[1m428/428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 39ms/step - accuracy: 0.4251 - loss: 1.4039 - val_accuracy: 0.4594 - val_loss: 1.3432 - learning_rate: 0.0010\n",
      "Epoch 4/25\n",
      "\u001b[1m428/428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 40ms/step - accuracy: 0.4536 - loss: 1.3407 - val_accuracy: 0.4765 - val_loss: 1.3035 - learning_rate: 0.0010\n",
      "Epoch 5/25\n",
      "\u001b[1m428/428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 41ms/step - accuracy: 0.4694 - loss: 1.3020 - val_accuracy: 0.5192 - val_loss: 1.2180 - learning_rate: 0.0010\n",
      "Epoch 6/25\n",
      "\u001b[1m428/428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 42ms/step - accuracy: 0.4757 - loss: 1.2686 - val_accuracy: 0.5248 - val_loss: 1.1991 - learning_rate: 0.0010\n",
      "Epoch 7/25\n",
      "\u001b[1m428/428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 41ms/step - accuracy: 0.4856 - loss: 1.2549 - val_accuracy: 0.5314 - val_loss: 1.1847 - learning_rate: 0.0010\n",
      "Epoch 8/25\n",
      "\u001b[1m428/428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 40ms/step - accuracy: 0.4987 - loss: 1.2252 - val_accuracy: 0.5123 - val_loss: 1.1995 - learning_rate: 0.0010\n",
      "Epoch 9/25\n",
      "\u001b[1m428/428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 40ms/step - accuracy: 0.5124 - loss: 1.1842 - val_accuracy: 0.5219 - val_loss: 1.1748 - learning_rate: 0.0010\n",
      "Epoch 10/25\n",
      "\u001b[1m428/428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 41ms/step - accuracy: 0.5188 - loss: 1.1763 - val_accuracy: 0.5261 - val_loss: 1.1595 - learning_rate: 0.0010\n",
      "Epoch 11/25\n",
      "\u001b[1m428/428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 41ms/step - accuracy: 0.5330 - loss: 1.1581 - val_accuracy: 0.5347 - val_loss: 1.1855 - learning_rate: 0.0010\n",
      "Epoch 12/25\n",
      "\u001b[1m428/428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 41ms/step - accuracy: 0.5427 - loss: 1.1373 - val_accuracy: 0.5400 - val_loss: 1.1647 - learning_rate: 0.0010\n",
      "Epoch 13/25\n",
      "\u001b[1m428/428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 41ms/step - accuracy: 0.5457 - loss: 1.1296 - val_accuracy: 0.5360 - val_loss: 1.1696 - learning_rate: 0.0010\n",
      "Epoch 14/25\n",
      "\u001b[1m428/428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 41ms/step - accuracy: 0.5493 - loss: 1.1155 - val_accuracy: 0.5327 - val_loss: 1.1580 - learning_rate: 0.0010\n",
      "Epoch 15/25\n",
      "\u001b[1m428/428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 41ms/step - accuracy: 0.5549 - loss: 1.1060 - val_accuracy: 0.5432 - val_loss: 1.1488 - learning_rate: 0.0010\n",
      "Epoch 16/25\n",
      "\u001b[1m428/428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 42ms/step - accuracy: 0.5634 - loss: 1.0812 - val_accuracy: 0.5400 - val_loss: 1.1538 - learning_rate: 0.0010\n",
      "Epoch 17/25\n",
      "\u001b[1m428/428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 41ms/step - accuracy: 0.5743 - loss: 1.0592 - val_accuracy: 0.5475 - val_loss: 1.1354 - learning_rate: 0.0010\n",
      "Epoch 18/25\n",
      "\u001b[1m428/428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 41ms/step - accuracy: 0.5759 - loss: 1.0617 - val_accuracy: 0.5406 - val_loss: 1.1924 - learning_rate: 0.0010\n",
      "Epoch 19/25\n",
      "\u001b[1m428/428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 48ms/step - accuracy: 0.5830 - loss: 1.0475 - val_accuracy: 0.5492 - val_loss: 1.1428 - learning_rate: 0.0010\n",
      "Epoch 20/25\n",
      "\u001b[1m428/428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 44ms/step - accuracy: 0.5856 - loss: 1.0464 - val_accuracy: 0.5541 - val_loss: 1.2037 - learning_rate: 0.0010\n",
      "Epoch 21/25\n",
      "\u001b[1m428/428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 44ms/step - accuracy: 0.5891 - loss: 1.0288 - val_accuracy: 0.5508 - val_loss: 1.2218 - learning_rate: 0.0010\n",
      "Epoch 22/25\n",
      "\u001b[1m428/428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 43ms/step - accuracy: 0.5893 - loss: 1.0272 - val_accuracy: 0.5505 - val_loss: 1.2140 - learning_rate: 0.0010\n",
      "Epoch 23/25\n",
      "\u001b[1m428/428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 44ms/step - accuracy: 0.5972 - loss: 1.0033 - val_accuracy: 0.5432 - val_loss: 1.2065 - learning_rate: 0.0010\n",
      "Epoch 24/25\n",
      "\u001b[1m428/428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 45ms/step - accuracy: 0.6120 - loss: 0.9806 - val_accuracy: 0.5584 - val_loss: 1.1846 - learning_rate: 0.0010\n",
      "Epoch 25/25\n",
      "\u001b[1m428/428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 44ms/step - accuracy: 0.6091 - loss: 0.9855 - val_accuracy: 0.5413 - val_loss: 1.2452 - learning_rate: 0.0010\n"
     ]
    }
   ],
   "source": [
    "rlrp = ReduceLROnPlateau(\n",
    "    monitor=\"loss\", factor=0.4, verbose=0, patience=2, min_lr=10e-8\n",
    ")\n",
    "history = model.fit(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    batch_size=64,\n",
    "    epochs=25,\n",
    "    validation_data=(x_test, y_test),\n",
    "    callbacks=[rlrp],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "138c0f51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "keras.src.models.sequential.Sequential"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "633b3098",
   "metadata": {},
   "source": [
    "### Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "dabc7e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_save_path = \"./saved_models/model.keras\"\n",
    "model.save(model_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "702deac4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.5522 - loss: 1.2399\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.245154619216919, 0.5412693023681641]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fd4039c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ser-training",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
